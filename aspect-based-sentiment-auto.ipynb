{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.1)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.10.31\n",
      "Collecting stanza\n",
      "  Using cached stanza-1.4.2-py3-none-any.whl (691 kB)\n",
      "Collecting emoji\n",
      "  Using cached emoji-2.2.0-py3-none-any.whl\n",
      "Collecting torch>=1.3.0\n",
      "  Using cached torch-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (890.2 MB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from stanza) (4.64.1)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.7/site-packages (from stanza) (3.19.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stanza) (1.21.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from stanza) (1.16.0)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.0->stanza) (4.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza) (59.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (3.4)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, emoji, nvidia-cudnn-cu11, torch, stanza\n",
      "Successfully installed emoji-2.2.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 stanza-1.4.2 torch-1.13.0\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.7/site-packages (2.2.0)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.14.0-cp37-cp37m-manylinux1_x86_64.whl (24.3 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (59.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Installing collected packages: tokenizers, sentencepiece, filelock, huggingface-hub, transformers, torchvision, sentence-transformers\n",
      "Successfully installed filelock-3.8.0 huggingface-hub-0.11.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 torchvision-0.14.0 transformers-4.24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip install nltk')\n",
    "os.system('pip install stanza')\n",
    "os.system('pip install emoji')\n",
    "os.system('pip install -U sentence-transformers')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "\n",
    "class Tweet():\n",
    "    def __init__(self, text, text_clean, token, author):\n",
    "        self.token = token\n",
    "        self.text = text\n",
    "        self.text_clean = text_clean\n",
    "        self.author = author\n",
    "\n",
    "        self.sentiments = {}\n",
    "        self.associations = []\n",
    "\n",
    "class User():\n",
    "    def __init__(self, author_id, handle, tweets):\n",
    "        self.author_id = author_id\n",
    "        self.handle = handle\n",
    "        self.tweets = {}\n",
    "        for x in tweets: self.tweets[x.token] = x\n",
    "\n",
    "#Removing Emojis\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', str(data))\n",
    "\n",
    "def cleaner(text):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",str(text)) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", str(text)) #Remove http links\n",
    "    tweet = re.sub('[()!?]', ' ', str(text)) #removing punctuation\n",
    "    tweet = re.sub('\\[.*?\\]',' ', str(text))\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(str(text))\n",
    "                     if w.lower() in words or not w.isalpha())\n",
    "    return text\n",
    "\n",
    "def calculate_sentiments(text, stop_words, nlp):\n",
    "    txt = text\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    featureList = []\n",
    "    categories = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        # Remove links from line\n",
    "        line = re.sub(r'http\\S+|#', '', line)\n",
    "\n",
    "        # Swap '-', ';', '*' with commas\n",
    "        line = re.sub(':', '.', line)\n",
    "        line = re.sub('\\n|@', '', line)\n",
    "\n",
    "        # Remove consecutive punctuation recursively\n",
    "        r = re.compile(r'([.,/#!$%^&*;:{}=_`~()-])[.,/#!$%^&*;:{}=_`~()-]+')\n",
    "        line = r.sub(r'\\1', line)\n",
    "\n",
    "        # Replace hashtags with association term\n",
    "        line = re.sub('#', 'hashtag is ', line)\n",
    "\n",
    "        try:\n",
    "            newtaggedList = []\n",
    "            txt_list = nltk.word_tokenize(line) # Splitting up into words\n",
    "            taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "            newwordList = []\n",
    "            flag = 0\n",
    "            for i in range(0,len(taggedList)-1):\n",
    "                if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                    newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "                    flag=1\n",
    "                else:\n",
    "                    if(flag==1):\n",
    "                        flag=0\n",
    "                        continue\n",
    "                    newwordList.append(taggedList[i][0])\n",
    "                    if(i==len(taggedList)-2):\n",
    "                        newwordList.append(taggedList[i+1][0])\n",
    "\n",
    "            finaltxt = ' '.join(word for word in newwordList)\n",
    "            new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "            wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "            taggedList = nltk.pos_tag(wordsList)\n",
    "\n",
    "            doc = nlp(finaltxt) # Object of Stanford NLP Pipeleine\n",
    "\n",
    "            dep_node = []\n",
    "\n",
    "            for dep_edge in doc.sentences[0].dependencies:\n",
    "                dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "            for i in range(0, len(dep_node)):\n",
    "                if (int(dep_node[i][1]) != 0):\n",
    "                    dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "            # featureList = []\n",
    "            # categories = []\n",
    "            for i in taggedList:\n",
    "                if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                    featureList.append(list(i))\n",
    "                    totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "                    categories.append(i[0])\n",
    "\n",
    "            for i in featureList:\n",
    "                filist = []\n",
    "                for j in dep_node:\n",
    "                    if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                        if(j[0]==i[0]):\n",
    "                            filist.append(j[1])\n",
    "                        else:\n",
    "                            filist.append(j[0])\n",
    "                fcluster.append([i[0], filist])\n",
    "\n",
    "        except IndexError:\n",
    "            # print('IndexError:', line)\n",
    "            return []\n",
    "\n",
    "        except AttributeError:\n",
    "            print('AttributeError')\n",
    "            return []\n",
    "\n",
    "    for i in featureList:\n",
    "        dic[i[0]] = i[1]\n",
    "\n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]]==\"NN\"):\n",
    "            finalcluster.append(i)\n",
    "\n",
    "    return finalcluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://sw-airlines-data-hub/data/processed/sw-airlines-tweets-w-users.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['tweet_clean'] = df['text'].str.lower().str.replace(r'[^0-9a-zA-Z\\s]+', '', regex=True).apply(cleaner)\n",
    "df['tweet_clean'] = df['tweet_clean'].apply(remove_emojis)\n",
    "X = df[['tweet_token', 'author_id','tweet_clean', 'text']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('en')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topics = ['book', 'cancel', 'call', 'support', 'delay', 'change', \n",
    "          'never', 'fear', 'pandemic', 'group', 'pilot', 'mask', 'avgeek', 'technology', 'pay']\n",
    "dfs = []\n",
    "for t in topics:\n",
    "    dfs.append(X[X.tweet_clean.str.contains(t)])\n",
    "    print(t, len(X[X.tweet_clean.str.contains(t)]))\n",
    "\n",
    "x0 = pd.concat(dfs)\n",
    "x0 = x0.set_index('tweet_token')\n",
    "x0['tweet_token'] = x0.index\n",
    "x0 = x0[['tweet_token', 'author_id', 'text', 'tweet_clean']]\n",
    "x0 = x0.drop_duplicates()\n",
    "len(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://sw-airlines-data-hub/data/processed/twt2twt_w_score.pkl...\n",
      "/ [1 files][ 34.0 MiB/ 34.0 MiB]                                                \n",
      "Operation completed over 1 objects/34.0 MiB.                                     \n",
      "Copying gs://sw-airlines-data-hub/data/processed/auth2auth_w_score.pkl...\n",
      "/ [1 files][  6.5 MiB/  6.5 MiB]                                                \n",
      "Operation completed over 1 objects/6.5 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://sw-airlines-data-hub/data/processed/twt2twt_w_score.pkl ./\n",
    "!gsutil cp gs://sw-airlines-data-hub/data/processed/auth2auth_w_score.pkl ./\n",
    "\n",
    "import pickle\n",
    "with open('twt2twt_w_score.pkl', 'rb') as f:\n",
    "    tweet_objs = pickle.load(f)\n",
    "    \n",
    "with open('auth2auth_w_score.pkl', 'rb') as f:\n",
    "    auth_objs = pickle.load(f)\n",
    "\n",
    "tweets = {} \n",
    "for x in tweet_objs: tweets[x.token] = x\n",
    "tweet_tokens = [x.token for x in tweet_objs]\n",
    "    \n",
    "authors = {} \n",
    "for x in auth_objs: authors[x.author_id] = x\n",
    "unique_authors = [x.author_id for x in auth_objs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr 1000 Duration: 0.176777    Estimated Remaining Time:  4 hours : 4 minutes.\n",
      "Itr 2000 Duration: 0.279321    Estimated Remaining Time:  4 hours : 11 minutes.\n",
      "Itr 3000 Duration: 0.078934    Estimated Remaining Time:  3 hours : 57 minutes.\n",
      "Itr 4000 Duration: 0.155101    Estimated Remaining Time:  3 hours : 53 minutes.\n",
      "Itr 5000 Duration: 0.195343    Estimated Remaining Time:  3 hours : 49 minutes.\n",
      "Itr 6000 Duration: 0.104038    Estimated Remaining Time:  3 hours : 46 minutes.\n",
      "Itr 7000 Duration: 0.201705    Estimated Remaining Time:  3 hours : 42 minutes.\n",
      "Itr 8000 Duration: 0.160117    Estimated Remaining Time:  3 hours : 38 minutes.\n",
      "Itr 9000 Duration: 0.227891    Estimated Remaining Time:  3 hours : 35 minutes.\n",
      "Itr 10000 Duration: 0.126333    Estimated Remaining Time:  3 hours : 31 minutes.\n",
      "Itr 11000 Duration: 0.105798    Estimated Remaining Time:  3 hours : 27 minutes.\n",
      "Itr 12000 Duration: 0.170477    Estimated Remaining Time:  3 hours : 24 minutes.\n",
      "Itr 13000 Duration: 0.297329    Estimated Remaining Time:  3 hours : 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentiments = {}\n",
    "length = len(tweet_tokens)\n",
    "import datetime\n",
    "import math\n",
    "completion_times = []\n",
    "i=0\n",
    "\n",
    "for tkn in tweet_tokens:\n",
    "    t1 = datetime.datetime.now()\n",
    "    tweet = tweets[tkn].text_clean\n",
    "    ttoken = tkn\n",
    "    associations = calculate_sentiments(tweet, stop_words, nlp).copy()\n",
    "    [tweets[tkn].associations.append(a) for a in associations.copy() if a[1]]\n",
    "    i+=1\n",
    "\n",
    "    # Calculate performance\n",
    "    t2 = datetime.datetime.now()\n",
    "    delta = t2 - t1\n",
    "    seconds = delta.total_seconds()\n",
    "    completion_times.append(seconds)\n",
    "    avg_s = round(sum(completion_times)/len(completion_times),2)\n",
    "    estimated_hours_left = (avg_s)*(length-i)/3600\n",
    "    hours_left = math.floor(estimated_hours_left)\n",
    "    minutes_left = math.floor((estimated_hours_left - hours_left)*60)\n",
    "    \n",
    "    if i % 1000 == 0: \n",
    "        print('Itr', i, 'Duration:', seconds, \"  \", 'Estimated Remaining Time: ',\n",
    "              hours_left,'hours :',minutes_left,'minutes.')\n",
    "        # break\n",
    "\n",
    "total_hours_needed = avg_s*length / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_objs = list(tweets.values())\n",
    "\n",
    "import pickle\n",
    "with open('twt2twt_w_score_w_sentiments.pkl', 'wb') as f:\n",
    "    pickle.dump(tweet_objs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_output(path, bucket_name, folder_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(folder_name + '/' + path.split('/')[-1])\n",
    "    blob.upload_from_filename(path)\n",
    "\n",
    "from google.cloud import storage\n",
    "bucket_name = 'sw-airlines-data-hub'\n",
    "upload_to_output('twt2twt_w_score_w_sentiments.pkl', bucket_name, 'data/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m97"
  },
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "local-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
