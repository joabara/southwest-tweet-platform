{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53778c59-ff8c-40bb-8e5b-6f3815b9b70e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.1)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.10.31\n",
      "Collecting stanza\n",
      "  Using cached stanza-1.4.2-py3-none-any.whl (691 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stanza) (1.21.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.7/site-packages (from stanza) (3.19.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from stanza) (4.64.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from stanza) (1.12.1)\n",
      "Collecting emoji\n",
      "  Using cached emoji-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.0->stanza) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (2.1.1)\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.2.0 stanza-1.4.2\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.7/site-packages (2.2.0)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.1)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.13.1+cu113)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Installing collected packages: tokenizers, sentencepiece, filelock, huggingface-hub, transformers, sentence-transformers\n",
      "Successfully installed filelock-3.8.0 huggingface-hub-0.11.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 40.9MB/s]                    \n",
      "2022-11-25 18:22:39 INFO: Downloading default packages for language: en (English) ...\n",
      "2022-11-25 18:22:42 INFO: File exists: /home/jupyter/stanza_resources/en/default.zip\n",
      "2022-11-25 18:22:51 INFO: Finished downloading models and saved to /home/jupyter/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip install nltk')\n",
    "os.system('pip install stanza')\n",
    "os.system('pip install emoji')\n",
    "os.system('pip install -U sentence-transformers')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "\n",
    "class Tweet():\n",
    "    def __init__(self, text, text_clean, token, author):\n",
    "        self.token = token\n",
    "        self.text = text\n",
    "        self.text_clean = text_clean\n",
    "        self.author = author\n",
    "\n",
    "        self.sentiments = {}\n",
    "        self.associations = []\n",
    "\n",
    "class User():\n",
    "    def __init__(self, author_id, handle, tweets):\n",
    "        self.author_id = author_id\n",
    "        self.handle = handle\n",
    "        self.tweets = {}\n",
    "        for x in tweets: self.tweets[x.token] = x\n",
    "\n",
    "#Removing Emojis\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', str(data))\n",
    "\n",
    "def cleaner(text):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",str(text)) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", str(text)) #Remove http links\n",
    "    tweet = re.sub('[()!?]', ' ', str(text)) #removing punctuation\n",
    "    tweet = re.sub('\\[.*?\\]',' ', str(text))\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(str(text))\n",
    "                     if w.lower() in words or not w.isalpha())\n",
    "    return text\n",
    "\n",
    "def calculate_sentiments(text, stop_words, nlp):\n",
    "    txt = text\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    featureList = []\n",
    "    categories = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        # Remove links from line\n",
    "        line = re.sub(r'http\\S+|#', '', line)\n",
    "\n",
    "        # Swap '-', ';', '*' with commas\n",
    "        line = re.sub(':', '.', line)\n",
    "        line = re.sub('\\n|@', '', line)\n",
    "\n",
    "        # Remove consecutive punctuation recursively\n",
    "        r = re.compile(r'([.,/#!$%^&*;:{}=_`~()-])[.,/#!$%^&*;:{}=_`~()-]+')\n",
    "        line = r.sub(r'\\1', line)\n",
    "\n",
    "        # Replace hashtags with association term\n",
    "        line = re.sub('#', 'hashtag is ', line)\n",
    "\n",
    "        try:\n",
    "            newtaggedList = []\n",
    "            txt_list = nltk.word_tokenize(line) # Splitting up into words\n",
    "            taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "            newwordList = []\n",
    "            flag = 0\n",
    "            for i in range(0,len(taggedList)-1):\n",
    "                if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                    newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "                    flag=1\n",
    "                else:\n",
    "                    if(flag==1):\n",
    "                        flag=0\n",
    "                        continue\n",
    "                    newwordList.append(taggedList[i][0])\n",
    "                    if(i==len(taggedList)-2):\n",
    "                        newwordList.append(taggedList[i+1][0])\n",
    "\n",
    "            finaltxt = ' '.join(word for word in newwordList)\n",
    "            new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "            wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "            taggedList = nltk.pos_tag(wordsList)\n",
    "\n",
    "            doc = nlp(finaltxt) # Object of Stanford NLP Pipeleine\n",
    "\n",
    "            dep_node = []\n",
    "\n",
    "            for dep_edge in doc.sentences[0].dependencies:\n",
    "                dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "            for i in range(0, len(dep_node)):\n",
    "                if (int(dep_node[i][1]) != 0):\n",
    "                    dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "            # featureList = []\n",
    "            # categories = []\n",
    "            for i in taggedList:\n",
    "                if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                    featureList.append(list(i))\n",
    "                    totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "                    categories.append(i[0])\n",
    "\n",
    "            for i in featureList:\n",
    "                filist = []\n",
    "                for j in dep_node:\n",
    "                    if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                        if(j[0]==i[0]):\n",
    "                            filist.append(j[1])\n",
    "                        else:\n",
    "                            filist.append(j[0])\n",
    "                fcluster.append([i[0], filist])\n",
    "\n",
    "        except IndexError:\n",
    "            print('IndexError:', line)\n",
    "            return []\n",
    "\n",
    "        except AttributeError:\n",
    "            print('AttributeError')\n",
    "            return []\n",
    "\n",
    "    for i in featureList:\n",
    "        dic[i[0]] = i[1]\n",
    "\n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]]==\"NN\"):\n",
    "            finalcluster.append(i)\n",
    "\n",
    "    return finalcluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37f78bc-f483-42c5-bf93-413ff5362335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://sw-airlines-data-hub/data/processed/twt2twt_w_score_w_sentiments.pkl...\n",
      "- [1 files][ 64.6 MiB/ 64.6 MiB]                                                \n",
      "Operation completed over 1 objects/64.6 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://sw-airlines-data-hub/data/processed/twt2twt_w_score_w_sentiments.pkl ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af61a9d-c650-44be-a18e-bf1d267188dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('twt2twt_w_score_w_sentiments.pkl', 'rb') as f:\n",
    "    twts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d245ca-020d-4719-a90d-d1be5cf1750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thanks']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twts[0].associations[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5704d259-cd73-4481-9094-98ae0d3d2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_dict = {}\n",
    "\n",
    "for t in twts:\n",
    "    for a in t.associations:\n",
    "        subject = a[0]\n",
    "        try: [sentiments_dict[subject].append(x) for x in a[1]]\n",
    "        except KeyError: \n",
    "            sentiments_dict[subject] = []\n",
    "            [sentiments_dict[subject].append(x) for x in a[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4383cd48-aa2e-4d3a-b1b3-68f0a4252ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from vaderSentiment) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (2.1.1)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d2385f-ffa3-45b9-8122-d4980f0c3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6d488b-cec2-4b6e-b22d-516e57e6aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentences = [x.text_clean for x in twts]\n",
    "sentiments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cce181f-2420-4ee8-bfeb-d5bd6606ee64",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aspect_df = pd.DataFrame(index=sentiments_dict.keys(), columns=['pos','neg','neu','compound', 'count'])\n",
    "\n",
    "for x in sentiments_dict.keys():\n",
    "    vs = analyzer.polarity_scores(' '.join(sentiments_dict[x]))\n",
    "    n = len(sentiments_dict[x]) \n",
    "    sentiments[x] = aspect_df.loc[x] = ([vs[x] for x in vs.keys()] + [n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8d44386-183b-4df5-bd0c-6ca88c15c9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>southwestair</th>\n",
       "      <td>0.145</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flight</th>\n",
       "      <td>0.181</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.9999</td>\n",
       "      <td>10834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>4144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decide</th>\n",
       "      <td>0.187</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>egainmasouthwestair</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ampsustainability</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sustainabilityexpert</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experteduardomariz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woce</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmentaviation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44628 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        pos    neg    neu compound  count\n",
       "southwestair          0.145  0.603  0.252      1.0  18111\n",
       "flight                0.181  0.707  0.111  -0.9999  10834\n",
       "airline               0.175  0.601  0.224   0.9998   4144\n",
       "decide                0.187  0.702  0.111  -0.0772     15\n",
       "egainmasouthwestair     0.0    1.0    0.0      0.0      1\n",
       "...                     ...    ...    ...      ...    ...\n",
       "ampsustainability       0.0    1.0    0.0      0.0      2\n",
       "sustainabilityexpert    0.0    1.0    0.0      0.0      1\n",
       "experteduardomariz      0.0    1.0    0.0      0.0      5\n",
       "woce                    0.0    1.0    0.0      0.0      2\n",
       "environmentaviation     0.0    1.0    0.0      0.0      1\n",
       "\n",
       "[44628 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1191a983-8ac0-45b4-9c6f-f1f9d48dd2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>southwestair</th>\n",
       "      <td>0.145</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>0.046</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>4144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travelaviation</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>3230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aviation</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>2934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.076</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cruise</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air</th>\n",
       "      <td>0.034</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.9914</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fly</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>1848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>0.061</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>1744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flightaircraft</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>1732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>1721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airport</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traveltrip</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>1580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.114</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip</th>\n",
       "      <td>0.072</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southwestairi</th>\n",
       "      <td>0.124</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aircraft</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thank</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>1261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seat</th>\n",
       "      <td>0.074</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>0.244</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>1144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0.162</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thing</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delta</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southwest</th>\n",
       "      <td>0.112</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>board</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.982</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avgeek</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.7073</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>0.054</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pos    neg    neu compound  count\n",
       "southwestair    0.145  0.603  0.252      1.0  18111\n",
       "travel          0.046   0.89  0.064   0.9967   4500\n",
       "airline         0.175  0.601  0.224   0.9998   4144\n",
       "travelaviation  0.006  0.956  0.039   0.9985   3230\n",
       "aviation        0.006  0.964   0.03   0.9963   2934\n",
       "time            0.076   0.82  0.103   0.9983   2833\n",
       "cruise          0.015  0.917  0.068   0.9991   2487\n",
       "air             0.034  0.909  0.057   0.9914   2017\n",
       "fly              0.05  0.838  0.112   0.9987   1848\n",
       "help            0.061  0.801  0.138   0.9992   1744\n",
       "flightaircraft  0.011  0.925  0.064   0.9977   1732\n",
       "book            0.019  0.835  0.146   0.9997   1721\n",
       "airport         0.053  0.868  0.079   0.9932   1689\n",
       "traveltrip      0.016  0.939  0.045   0.9933   1580\n",
       "way             0.114  0.541  0.344   0.9999   1420\n",
       "trip            0.072  0.756  0.172   0.9995   1321\n",
       "southwestairi   0.124  0.643  0.234   0.9993   1289\n",
       "aircraft         0.01   0.96   0.03   0.9735   1281\n",
       "thank           0.014  0.875  0.111   0.9989   1261\n",
       "seat            0.074   0.71  0.216   0.9996   1220\n",
       "service         0.244  0.496   0.26   0.9919   1144\n",
       "day             0.162  0.507  0.331   0.9998   1118\n",
       "thing            0.13  0.653  0.217   0.9988   1009\n",
       "use             0.044  0.807   0.15   0.9983    970\n",
       "delta           0.039  0.868  0.093   0.9841    938\n",
       "southwest       0.112  0.666  0.222   0.9992    914\n",
       "board           0.069  0.849  0.082   0.6908    873\n",
       "pay             0.069   0.81   0.12    0.982    860\n",
       "avgeek            0.0  0.994  0.006   0.7073    810\n",
       "change          0.054  0.855  0.091   0.9667    800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_df[aspect_df.compound > 0.50].sort_values(by='count' , ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b73e6d34-41f3-48b5-9a4b-56e58537adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "\n",
    "top10_sentiments_per_aspect = {}\n",
    "\n",
    "for x in sentiments_dict.keys(): top10_sentiments_per_aspect[x] = Counter(sentiments_dict[x]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3222f965-dd6b-4ec8-beb2-da145a5d27f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('southwestair', 39),\n",
       " ('airlines', 38),\n",
       " ('great', 34),\n",
       " ('worst', 32),\n",
       " ('terrible', 24),\n",
       " ('southwestaircustomer', 24),\n",
       " ('horrible', 24),\n",
       " ('called', 23),\n",
       " ('poor', 23),\n",
       " ('best', 21)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_sentiments_per_aspect['customerservice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16704b80-32fb-4073-aa73-db78f261d7be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "southwestair i have called customer service 3 times about an issue with a voucher and i keep getting the same response and no resolution who can i contact to get my voucher\n",
      "----------\n",
      "southwestair thought for sure this was a mistake called your customer service  to get it straightened out and quentin proceeds to talk to me like im an idiot i ask to speak to his manager he says it will be at least 90 min so i will give up\n",
      "----------\n",
      "southwestair i purchased my flight about 5 months ago i added early bird i did my part my flight leaves tomorrow and i got b53 position seems like early bird is a scam i called customer service and was told that swa is sorry for the inconvenience\n",
      "----------\n",
      "southwestair earlybird ck in on app didnt work for both of my flights  called customer service both times and was put on hold 40 minutes them disconnected where is the service you so proudly advertise all the time\n",
      "----------\n",
      "southwestair my bag is in san jose i cant get there and cant pay for the shipping to my home due to it not being at the place i was rerouted to \n",
      "\n",
      "ive called the customer service the cbs and talked to the baggage person at san jose tho has my bag\n",
      "\n",
      "please help\n",
      "----------\n",
      "southwestair my flight was just rebooked till tomorrow i called customer service and they refused to pay for a hotel or rebook me on any earlier flight\n",
      "----------\n",
      "southwestair i am a customer of size taking my second trip  when i traveled last year the agent i spoke to treated me like she cared and booked my extra seat without charging me i booked my second flight then called customer service and was fat shamed by the rep not flying\n",
      "----------\n",
      "southwestair i called customer service and they literally said the airport i flew into should have made the report so id get my bags tomorrow instead i have to figure out getting to a different airport i dont have a car to get my bag southwestair is the worst\n",
      "----------\n",
      "southwestair  have an unexpected flight friday and need my name change to married last name called customer service and was not successful help\n",
      "----------\n",
      "southwestair well done you cancel my flight yesterday rebooked me and now my flight is delayed called customer service and was told i am expected to sleep at the airport because missing the connecting flight is not a legitimate reason for accommodation shame on you sw\n",
      "----------\n",
      "southwestair i have called customer service numerous times emailed and nothing the customer service on phone infact was super rude i dont want to type the whole situation if you are not willing to give me a better compensation\n",
      "----------\n",
      "southwestair i have called customer service numerous times emailed and nothing the customer service on phone infact was super rude i dont want to type the whole situation if you are not willing to give me a better compensation\n",
      "----------\n",
      "southwestair i tried being a ticket on weds of last week and you guys took my money but no ticket was received i called customer service and they said to call my bank and the bank told me you guys did indeed take my money there is a pending transaction\n",
      "----------\n",
      "southwestair i use you guys religiously and that might stop soon  i was promised a luv voucher for my soon to expire credits and i have yet to receive them   called customer service and still no help  so disappointed and time to look at other airlines\n",
      "----------\n",
      "southwestair once again my flight has been delayed for the third time this week when i called customer service your agent wasnt helpful at all and couldnt get me on an earlier flight even though it was available on the website extremely disappointed by this\n",
      "----------\n",
      "southwestair you lost my luggage today on a direct flight i called customer service they assured my luggage would be delivered next day now i am told it will be two days they were extremely rude and unhelpful completely unprofessional i need my luggage now unacceptable\n",
      "----------\n",
      "i was having trouble accessing a flight credit online so i called customer service a lady named tammyac answered amp was sooo helpful you guys really do have some of the best staff memebers southwestair\n",
      "----------\n",
      "southwestair charged my card twice and did not book a ticket called customer service and they basically said who cares just call your bank now my daughter will miss her grandparent funeral ill never fly southwest again no compassion\n",
      "----------\n",
      "it was quite uncomfortable when the gate agent followed me down the jetway to make sure i didnt carry my banner onto the plane i called southwestair customer service who said i could carry it on as well as my 2 items i was fine with checking my banner but dont follow me\n",
      "----------\n",
      "was left abandoned by southwestair in liberia cr on aug 11th called customer service and was told to submit a complaint online 51808444 no response as of today \n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for x in twts:\n",
    "    for a in x.associations:\n",
    "        if 'customerservice' in a and 'called' in a[1]:\n",
    "            print(x.text_clean)\n",
    "            print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c9a166-e666-4ecd-92d3-2c317034a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 s, sys: 30.3 ms, total: 17.2 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweet_sentiments = {}\n",
    "for x in twts:\n",
    "    vs = analyzer.polarity_scores(x.text_clean)\n",
    "    tweet_sentiments[x.token] = ([vs[y] for y in vs.keys()] + [x.text_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d681e557-8b8e-456d-bf55-37d7bdd8f05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 'southwestair addressed  all i received was an automated email saying customer relations would contact me over a month ago']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_sentiments[twts[0].token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c52cb8f-deee-4d1e-a5c8-b49b9dbf89e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>is_neg</th>\n",
       "      <th>is_pos</th>\n",
       "      <th>tweet_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1589998047110782976</th>\n",
       "      <td>0.289</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>southwestair thanks ill do that now</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1589998047110782976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589997663336148993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>southwestair we have a flight from norfolk to ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1589997663336148993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589996983066189824</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aviatorjlat egainma southwestair usdot nonuttr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1589996983066189824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589996585316147200</th>\n",
       "      <td>0.079</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kaoconnor southwestair denairport midwayairpor...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1589996585316147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589996464733769728</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>good morning southwestair do we get free alcoh...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1589996464733769728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pos    neg    neu compound  \\\n",
       "1589998047110782976  0.289  0.412  0.299   0.0258   \n",
       "1589997663336148993    0.0    1.0    0.0      0.0   \n",
       "1589996983066189824    0.0    1.0    0.0      0.0   \n",
       "1589996585316147200  0.079  0.842  0.079      0.0   \n",
       "1589996464733769728  0.071  0.612  0.316   0.7003   \n",
       "\n",
       "                                                            text_clean  \\\n",
       "1589998047110782976               southwestair thanks ill do that now    \n",
       "1589997663336148993  southwestair we have a flight from norfolk to ...   \n",
       "1589996983066189824  aviatorjlat egainma southwestair usdot nonuttr...   \n",
       "1589996585316147200  kaoconnor southwestair denairport midwayairpor...   \n",
       "1589996464733769728  good morning southwestair do we get free alcoh...   \n",
       "\n",
       "                     is_neg  is_pos          tweet_token  \n",
       "1589998047110782976   False   False  1589998047110782976  \n",
       "1589997663336148993   False   False  1589997663336148993  \n",
       "1589996983066189824   False   False  1589996983066189824  \n",
       "1589996585316147200   False   False  1589996585316147200  \n",
       "1589996464733769728   False    True  1589996464733769728  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_aspect_df = pd.DataFrame(index=tweet_sentiments.keys(), columns=['pos','neg','neu','compound', 'text_clean'])\n",
    "\n",
    "for token in tweet_sentiments.keys(): tweet_aspect_df.loc[token] = tweet_sentiments[token]\n",
    "tweet_aspect_df['is_neg'] = tweet_aspect_df.compound.apply(lambda x: True if x < -0.50 else False)\n",
    "tweet_aspect_df['is_pos'] = tweet_aspect_df.compound.apply(lambda x: True if x > 0.50 else False)\n",
    "tweet_aspect_df['tweet_token'] = tweet_aspect_df.index\n",
    "tweet_aspect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66e314e9-3dc3-47c9-8ef3-bb2c1bebeb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://sw-airlines-data-hub/data/processed/tweet2tweet_df3.pickle...\n",
      "- [1 files][ 68.5 MiB/ 68.5 MiB]                                                \n",
      "Operation completed over 1 objects/68.5 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://sw-airlines-data-hub/data/processed/tweet2tweet_df3.pickle ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d615eda-7dfc-4eb7-bccb-6667802d7ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 199727 entries, 6 to 1111606\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   TweetTokenA      199727 non-null  object \n",
      " 1   TweetTokenB      199727 non-null  object \n",
      " 2   SimilarityScore  199727 non-null  float64\n",
      " 3   AspectsA         199727 non-null  object \n",
      " 4   AspectsB         199727 non-null  object \n",
      " 5   AuthorTokenA     199727 non-null  int64  \n",
      " 6   UserA            198814 non-null  object \n",
      " 7   text_x           199727 non-null  object \n",
      " 8   a_created_at     199727 non-null  object \n",
      " 9   a_rt_cnt         199727 non-null  int64  \n",
      " 10  a_reply_cnt      199727 non-null  int64  \n",
      " 11  a_like_count     199727 non-null  int64  \n",
      " 12  a_qt_count       199727 non-null  int64  \n",
      " 13  AuthorTokenB     199727 non-null  int64  \n",
      " 14  UserB            199313 non-null  object \n",
      " 15  text_y           199727 non-null  object \n",
      " 16  b_created_at     199727 non-null  object \n",
      " 17  b_rt_cnt         199727 non-null  int64  \n",
      " 18  b_reply_cnt      199727 non-null  int64  \n",
      " 19  b_like_count     199727 non-null  int64  \n",
      " 20  b_qt_count       199727 non-null  int64  \n",
      "dtypes: float64(1), int64(10), object(10)\n",
      "memory usage: 33.5+ MB\n"
     ]
    }
   ],
   "source": [
    "t2t = pd.read_pickle('tweet2tweet_df3.pickle')\n",
    "t2t.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379edf78-cbeb-4feb-91df-3272b8ecdf6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199727"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match A Sentiments\n",
    "t2t = t2t.merge(tweet_aspect_df, left_on='TweetTokenA', right_on='tweet_token')\n",
    "t2t = t2t.rename(columns={'text_clean': 'text_clean_A',\n",
    "                          'pos': 'A_pos_score', \n",
    "                          'neg' : 'A_neg_score', \n",
    "                          'neu':'A_neutral_score', \n",
    "                          'compound':'A_compound_score', \n",
    "                          'is_neg':'A_is_neg',\n",
    "                          'is_pos':'A_is_pos'})\n",
    "t2t = t2t.drop(columns=['tweet_token'])\n",
    "\n",
    "# Match B Sentiments\n",
    "t2t = t2t.merge(tweet_aspect_df, left_on='TweetTokenB', right_on='tweet_token')\n",
    "t2t = t2t.rename(columns={'text_clean': 'text_clean_B',\n",
    "                          'pos': 'B_pos_score', \n",
    "                          'neg' : 'B_neg_score', \n",
    "                          'neu':'B_neutral_score', \n",
    "                          'compound':'B_compound_score', \n",
    "                          'is_neg':'B_is_neg',\n",
    "                          'is_pos':'B_is_pos'})\n",
    "\n",
    "t2t = t2t.drop(columns=['tweet_token'])\n",
    "len(t2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e14211-1a5c-46bb-8c50-dba4c0fa4e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetTokenA</th>\n",
       "      <th>TweetTokenB</th>\n",
       "      <th>SimilarityScore</th>\n",
       "      <th>AspectsA</th>\n",
       "      <th>AspectsB</th>\n",
       "      <th>AuthorTokenA</th>\n",
       "      <th>UserA</th>\n",
       "      <th>text_x</th>\n",
       "      <th>a_created_at</th>\n",
       "      <th>a_rt_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>text_clean_A</th>\n",
       "      <th>a_is_neg</th>\n",
       "      <th>a_is_pos</th>\n",
       "      <th>pos_score_b</th>\n",
       "      <th>neg_score_b</th>\n",
       "      <th>neutral_score_b</th>\n",
       "      <th>compound_score_b</th>\n",
       "      <th>text_clean_B</th>\n",
       "      <th>b_is_neg</th>\n",
       "      <th>b_is_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1583825720316305408</td>\n",
       "      <td>1589953501517737985</td>\n",
       "      <td>0.890007</td>\n",
       "      <td>[['golf', ['clubs']], ['golftrip', ['triptoday...</td>\n",
       "      <td>[]</td>\n",
       "      <td>215716356</td>\n",
       "      <td>The_Real_FKD</td>\n",
       "      <td>So I fly with my golf clubs pretty consistentl...</td>\n",
       "      <td>2022-10-22T14:20:52.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>so i fly with my golf clubs pretty consistentl...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ryanfoxy24 aircanada southwestair im going to ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1589953491040366592</td>\n",
       "      <td>1592671061837221889</td>\n",
       "      <td>0.892296</td>\n",
       "      <td>[['southwestairshame', ['control']], ['control...</td>\n",
       "      <td>[['seat', ['own', 'choosing']], ['idea', ['gre...</td>\n",
       "      <td>1416721475424464898</td>\n",
       "      <td>JimMcneese</td>\n",
       "      <td>@SouthwestAir Shame on you once again.. canâ€™t ...</td>\n",
       "      <td>2022-11-08T12:10:26.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>southwestair shame on you once again cant cont...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>neveragain i used to love traveling on southwe...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1576613811477393409</td>\n",
       "      <td>1592671061837221889</td>\n",
       "      <td>0.894714</td>\n",
       "      <td>[['seatingsystem', ['dislike']], ['seat', ['ch...</td>\n",
       "      <td>[['seat', ['own', 'choosing']], ['idea', ['gre...</td>\n",
       "      <td>1126752147620544512</td>\n",
       "      <td>JoshuaBigler</td>\n",
       "      <td>I love @SouthwestAirâ€™s customer service and la...</td>\n",
       "      <td>2022-10-02T16:43:19.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>i love southwestairs customer service and lack...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>neveragain i used to love traveling on southwe...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1569153634897924096</td>\n",
       "      <td>1592671061837221889</td>\n",
       "      <td>0.882039</td>\n",
       "      <td>[['southwestair', ['wanted']], ['let', ['wante...</td>\n",
       "      <td>[['seat', ['own', 'choosing']], ['idea', ['gre...</td>\n",
       "      <td>493689130</td>\n",
       "      <td>WaifusaurusRex</td>\n",
       "      <td>@SouthwestAir just wanted to let someone in yo...</td>\n",
       "      <td>2022-09-12T02:39:14.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>southwestair just wanted to let someone in you...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>neveragain i used to love traveling on southwe...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1591637809877901313</td>\n",
       "      <td>1592671061837221889</td>\n",
       "      <td>0.894431</td>\n",
       "      <td>[['feel', ['used', 'valued', 'somehow']], ['pa...</td>\n",
       "      <td>[['seat', ['own', 'choosing']], ['idea', ['gre...</td>\n",
       "      <td>1337825236973056001</td>\n",
       "      <td>TravelsWithMic2</td>\n",
       "      <td>I really miss the days when #airlines cared ab...</td>\n",
       "      <td>2022-11-13T03:43:19.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>i really miss the days when airlines cared abo...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>neveragain i used to love traveling on southwe...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           TweetTokenA          TweetTokenB  SimilarityScore  \\\n",
       "0  1583825720316305408  1589953501517737985         0.890007   \n",
       "1  1589953491040366592  1592671061837221889         0.892296   \n",
       "2  1576613811477393409  1592671061837221889         0.894714   \n",
       "3  1569153634897924096  1592671061837221889         0.882039   \n",
       "4  1591637809877901313  1592671061837221889         0.894431   \n",
       "\n",
       "                                            AspectsA  \\\n",
       "0  [['golf', ['clubs']], ['golftrip', ['triptoday...   \n",
       "1  [['southwestairshame', ['control']], ['control...   \n",
       "2  [['seatingsystem', ['dislike']], ['seat', ['ch...   \n",
       "3  [['southwestair', ['wanted']], ['let', ['wante...   \n",
       "4  [['feel', ['used', 'valued', 'somehow']], ['pa...   \n",
       "\n",
       "                                            AspectsB         AuthorTokenA  \\\n",
       "0                                                 []            215716356   \n",
       "1  [['seat', ['own', 'choosing']], ['idea', ['gre...  1416721475424464898   \n",
       "2  [['seat', ['own', 'choosing']], ['idea', ['gre...  1126752147620544512   \n",
       "3  [['seat', ['own', 'choosing']], ['idea', ['gre...            493689130   \n",
       "4  [['seat', ['own', 'choosing']], ['idea', ['gre...  1337825236973056001   \n",
       "\n",
       "             UserA                                             text_x  \\\n",
       "0     The_Real_FKD  So I fly with my golf clubs pretty consistentl...   \n",
       "1       JimMcneese  @SouthwestAir Shame on you once again.. canâ€™t ...   \n",
       "2     JoshuaBigler  I love @SouthwestAirâ€™s customer service and la...   \n",
       "3   WaifusaurusRex  @SouthwestAir just wanted to let someone in yo...   \n",
       "4  TravelsWithMic2  I really miss the days when #airlines cared ab...   \n",
       "\n",
       "               a_created_at  a_rt_cnt  ...  \\\n",
       "0  2022-10-22T14:20:52.000Z         0  ...   \n",
       "1  2022-11-08T12:10:26.000Z         0  ...   \n",
       "2  2022-10-02T16:43:19.000Z         0  ...   \n",
       "3  2022-09-12T02:39:14.000Z         0  ...   \n",
       "4  2022-11-13T03:43:19.000Z         1  ...   \n",
       "\n",
       "                                        text_clean_A  a_is_neg  a_is_pos  \\\n",
       "0  so i fly with my golf clubs pretty consistentl...     False     False   \n",
       "1  southwestair shame on you once again cant cont...     False      True   \n",
       "2  i love southwestairs customer service and lack...     False     False   \n",
       "3  southwestair just wanted to let someone in you...      True     False   \n",
       "4  i really miss the days when airlines cared abo...     False     False   \n",
       "\n",
       "   pos_score_b neg_score_b neutral_score_b compound_score_b  \\\n",
       "0          0.0         1.0             0.0              0.0   \n",
       "1        0.087       0.787           0.126           0.0387   \n",
       "2        0.087       0.787           0.126           0.0387   \n",
       "3        0.087       0.787           0.126           0.0387   \n",
       "4        0.087       0.787           0.126           0.0387   \n",
       "\n",
       "                                        text_clean_B  b_is_neg  b_is_pos  \n",
       "0  ryanfoxy24 aircanada southwestair im going to ...     False     False  \n",
       "1  neveragain i used to love traveling on southwe...     False     False  \n",
       "2  neveragain i used to love traveling on southwe...     False     False  \n",
       "3  neveragain i used to love traveling on southwe...     False     False  \n",
       "4  neveragain i used to love traveling on southwe...     False     False  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1d6efb0-2272-4ffa-a0e7-cc335840041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t.to_pickle('tweet2tweet_df4.pickle')\n",
    "\n",
    "def upload_to_output(path, bucket_name, folder_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(folder_name + '/' + path.split('/')[-1])\n",
    "    blob.upload_from_filename(path)\n",
    "\n",
    "from google.cloud import storage   \n",
    "bucket_name = 'sw-airlines-data-hub'\n",
    "upload_to_output('tweet2tweet_df4.pickle', bucket_name, 'data/processed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
